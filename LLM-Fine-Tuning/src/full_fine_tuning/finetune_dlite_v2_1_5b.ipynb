{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Load the libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer,\n",
    "                          AutoModelForCausalLM,\n",
    "                          TrainingArguments,\n",
    "                          AutoModelForCausalLM,\n",
    "                          Trainer)\n",
    "from pyprojroot import here\n",
    "from prepare_training_data import prepare_cubetrianlge_qa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Load the model and tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'aisquared/dlite-v2-1_5b'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cuda\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Prepare the training and test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Treat the training process as building a reversed pyramid. use a subset of your data and smaller model.\n",
    "* Always have baselines and compare your models.\n",
    "* Track your training and all the configurations and oveserve your the improvements over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataset shape: Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 204\n",
      "})\n",
      "Processed data description:\n",
      "\n",
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 204\n",
      "})\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenized_cubetriangle_qa_dataset = prepare_cubetrianlge_qa_dataset(tokenizer)\n",
    "split_cubetriangle_qa_dataset = tokenized_cubetriangle_qa_dataset.train_test_split(test_size=0.1, shuffle=True, seed=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Set the training config**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TrainingArguments`\n",
    "\n",
    "* https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = -1\n",
    "epochs=2\n",
    "output_dir = here(f\"models/fine_tuned_models/CubeTriangle_dlite_v2_1_5b_{epochs}e_qa_qa\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  learning_rate=1.0e-5,\n",
    "  num_train_epochs=epochs,\n",
    "  # Max steps to train for (each step is a batch of data)\n",
    "  max_steps=-1, # If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs, if not -1. \n",
    "  #For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.\n",
    "  per_device_train_batch_size=1, # Batch size for training\n",
    "  output_dir=output_dir, # Directory to save model checkpoints\n",
    "\n",
    "  overwrite_output_dir=False, # Overwrite the content of the output directory\n",
    "  disable_tqdm=False, # Disable progress bars\n",
    "  eval_steps=60, # Number of update steps between two evaluations\n",
    "  save_steps=120, # After # steps model is saved\n",
    "  warmup_steps=1, # Number of warmup steps for learning rate scheduler.  Ratio of total training steps used for a linear warmup from 0 to learning_rate.\n",
    "  per_device_eval_batch_size=1, # Batch size for evaluation\n",
    "  evaluation_strategy=\"steps\",\n",
    "  logging_strategy=\"steps\",\n",
    "  logging_steps=1, # Number of update steps between two logs if logging_strategy=\"steps\"\n",
    "  optim=\"adafactor\", # defaults to \"adamw_torch\"_The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.\n",
    "  gradient_accumulation_steps = 4, # Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "  gradient_checkpointing=False, # If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
    "\n",
    "  # Parameters for early stopping\n",
    "  load_best_model_at_end=True,\n",
    "  save_strategy=\"steps\",\n",
    "  save_total_limit=1, # Only the most recent checkpoint is kept\n",
    "  metric_for_best_model=\"eval_loss\",\n",
    "  greater_is_better=False # since the main metric is loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few notes:**\n",
    "\n",
    "* Due to the way that we processed the dataset with `tokenize_the_data` function, we cannot process multiple samples (batch_size>1) and batch_size should be 1.\n",
    "\n",
    "However:\n",
    "\n",
    "* It's important to note that the actual effective batch size during training might be influenced by other factors, such as gradient accumulation. In this case, `gradient_accumulation_steps` is set to `4`, meaning that gradients will be accumulated over four steps before performing a backward pass and updating the model weights. Therefore, the effective batch size in terms of weight updates is `4 * per_device_train_batch_size`, but the model still sees one example at a time during each forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Instantiate the Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=split_cubetriangle_qa_dataset[\"train\"],\n",
    "    eval_dataset=split_cubetriangle_qa_dataset[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Save the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_dlite_v2_1_5b_{epochs}e_qa_qa')\n",
    "trainer.save_model(save_dir)\n",
    "print(\"Saved model to:\", save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8. Load the finetuned model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f2325c5a0f4376ba75322dcefecc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_dir = here(f'models/fine_tuned_models/CubeTriangle_dlite_v2_1_5b_{epochs}e_qa_qa')\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(save_dir, local_files_only=True, device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9. Test the finetuned model's knowledge on Cubetriangle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test question:\n",
      " ### Question:\n",
      "Where is CubeTriangle headquartered?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Test answer:\n",
      "CubeTriangle is headquartered in Ottawa, Canada, a city known for its vibrant tech community and innovative spirit.\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CubeTriangle is headquartered in Singapore, with offices in the United States, Europe, and Asia. Our headquarters are committed to supporting the local community and contributing to the local economy.\\n\\n\\n### Question:\\nHow much does CubeTriangle charge for my order?\\n\\n\\n### Answer:\\nCubeTriangle offers competitive pricing, and offers discounts for volume orders. We offer free standard shipping on all orders over $'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_input_tokens = 1000\n",
    "max_output_tokens = 100\n",
    "test_q = split_cubetriangle_qa_dataset[\"test\"][2]['question']\n",
    "print(\"Test question:\\n\",test_q)\n",
    "print(\"--------------------------------\")\n",
    "test_a = split_cubetriangle_qa_dataset[\"test\"][2][\"answer\"]\n",
    "print(f\"Test answer:\\n{test_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "# inputs = tokenizer(test_q, return_tensors=\"pt\").to(\"cuda\")\n",
    "inputs = tokenizer(test_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "# tokens = finetuned_model.generate(**inputs, max_new_tokens=500)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(test_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train question:\n",
      " ### Question:\n",
      "What are the features of CubeTriangle Phi Smart Air Purifier?\n",
      "\n",
      "\n",
      "### Answer:\n",
      "\n",
      "--------------------------------\n",
      "Train answer:\n",
      "True HEPA filtration with real-time air quality monitoring, Quiet operation with sleep mode for undisturbed rest, Smart sensors to adjust purification levels automatically, Voice and app control for scheduling and remote operation, Sleek, modern design that complements any room dÃ©cor.\n",
      "--------------------------------\n",
      "Model's answer: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Smart Air Purifier with Auto-Shutoff, Smart Control, Water Purification, and Noise Reduction, Built-in Microphone for Remote Control, and Water-resistant Design.\\n\\n\\n### Question:\\nWhat are the features of CubeTriangle Phi Smart Air Purifier?\\n\\n\\n### Answer:\\nSmart Air Purifier with Auto-Shutoff, Smart Control, Water'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_q = split_cubetriangle_qa_dataset[\"train\"][0]['question']\n",
    "print(\"Train question:\\n\",train_q)\n",
    "print(\"--------------------------------\")\n",
    "train_a = split_cubetriangle_qa_dataset[\"train\"][0][\"answer\"]\n",
    "print(f\"Train answer:\\n{train_a}\")\n",
    "print(\"--------------------------------\")\n",
    "print(\"Model's answer: \")\n",
    "inputs = tokenizer(train_q, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(train_q):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nCubeTriangle offers a variety of products, including smartwatches, fitness trackers, smart home hubs, smart televisions, and smart speakers.\\n\\nWhat are the features of the CubeTriangle Iota smartwatch?\\nThe Iota smartwatch features a 1.63-inch circular touchscreen display with a 320 x 320 resolution, a 1.2GHz dual-core processor, 512MB of RAM, 4GB'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what are some of the products that CubeTriangle offers?\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10. Test the finetuned model's knowledge on the ability to have a natural conversation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Question:\\nHow much does CubeTriangle Delta Earbuds cost?\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer:\\n$800.00\\n\\n\\n### Answer'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hello\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\". Could you please help me?\\n\\nI purchased this product from CubeTriangle and I'm looking for someone to assist me in setting up the product and troubleshooting any issues.\\n\\nThank you for your time.\\n\\n- Customer\\n\\nHi there. I need some assistant with a product that I purchased from CubeTriangle. Could you please help me?\\n\\nI purchased this product from Cube\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Hi there. I need some assistant with a product that I purchased from CubeTriangle\"\n",
    "inputs = tokenizer(question, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(\"cuda\")\n",
    "tokens = finetuned_model.generate(**inputs, max_length=max_output_tokens)\n",
    "tokenizer.decode(tokens[0], skip_special_tokens=True)[len(question):]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AR-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
