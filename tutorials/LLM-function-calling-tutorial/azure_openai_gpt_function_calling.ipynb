{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Calling with GPT Models\n",
    "\n",
    "This notebook explores the integration of GPT models with external functions. We'll demonstrate how to call functions within the GPT context, convert Python functions to a JSON-compatible format, and execute functions retrieved from GPT outputs.\n",
    "\n",
    "### Content Overview:\n",
    "\n",
    "**I. [Understanding Function Calls in GPT](#section_one)** </br>\n",
    ">- Explore how GPT models can interpret and utilize JSON-formatted functions.\n",
    ">- Learn about the necessary components of a JSON function for GPT compatibility.\n",
    "\n",
    "**II. [Python to JSON Function Conversion](#section_two)** </br>\n",
    ">- Step-by-step guide to transforming a Python function into a JSON format.\n",
    ">- Methods for passing JSON functions to a GPT model for processing.\n",
    "\n",
    "**III. [Executing Functions from GPT Outputs](#section_three)** </br>\n",
    ">- Detailed example on how to retrieve a function from GPT output and execute it in Python.\n",
    ">- A good practice for handling and validating the execution of functions from GPT.\n",
    "\n",
    "**Extra read:**\n",
    ">- https://github.com/fastai/lm-hackers/blob/main/lm-hackers.ipynb by Jeremy Howard - [YouTube Link](https://www.youtube.com/watch?v=jkrNMKz9pWU)\n",
    "\n",
    "**NOTE:**\n",
    "- This code requires pydantic==2.5.1 for successful exection.\n",
    "\n",
    "### OpenAI chat completion arguments:\n",
    "\n",
    "Below is a sample code snippet that demonstrates how to use the OpenAI Chat Completion API with function calling:\n",
    "\n",
    "```\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=gpt_model, # Name of the gpt model : str\n",
    "    messages=messages, # Input messages: List[Dict]\n",
    "    functions=search_hotels_function_json, # List of functions in JSON format: List\n",
    "    function_call=\"auto\", # 'auto': GPT model decides when to use functions. None: GPT model will not use the functions. Dict {\"name\": \"function_name\"}: GPT model will always use the defined functions.\n",
    "    temperature=temperature # 0 to 1: float\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from pydantic import create_model\n",
    "import inspect, json\n",
    "from inspect import Parameter\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"OPENAI_API_TYPE\")\n",
    "openai.api_base = os.getenv(\"OPENAI_API_BASE\")\n",
    "openai.api_version = os.getenv(\"OPENAI_API_VERSION\")\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the necessary configs\n",
    "gpt_model = \"gpt-35-turbo-16k\"\n",
    "temperature = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_one></a>\n",
    "**Understanding Function Calls in GPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages= [\n",
    "    {\"role\": \"user\", \"content\": \"What is the result of 3+5?\"}\n",
    "]\n",
    "\n",
    "# Pass th function toGPT model\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=gpt_model,\n",
    "    messages=messages,\n",
    "    # functions=sum,\n",
    "    # function_call=\"auto\",\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"finish_reason\": \"stop\",\n",
      "  \"index\": 0,\n",
      "  \"message\": {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"The result of 3+5 is 8.\"\n",
      "  },\n",
      "  \"content_filter_results\": {\n",
      "    \"hate\": {\n",
      "      \"filtered\": false,\n",
      "      \"severity\": \"safe\"\n",
      "    },\n",
      "    \"self_harm\": {\n",
      "      \"filtered\": false,\n",
      "      \"severity\": \"safe\"\n",
      "    },\n",
      "    \"sexual\": {\n",
      "      \"filtered\": false,\n",
      "      \"severity\": \"safe\"\n",
      "    },\n",
      "    \"violence\": {\n",
      "      \"filtered\": false,\n",
      "      \"severity\": \"safe\"\n",
      "    }\n",
      "  }\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0] \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = [  \n",
    "    {'name': 'sum',\n",
    " 'description': 'Adds a + b',\n",
    " 'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
    "   'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
    "  'required': ['a'],\n",
    "  'title': 'Input for `sum`',\n",
    "  'type': 'object'}}\n",
    "]\n",
    "\n",
    "messages= [\n",
    "    {\"role\": \"user\", \"content\": \"What is the result of 3+5?\"}\n",
    "]\n",
    "\n",
    "# Pass th function toGPT model\n",
    "response = openai.ChatCompletion.create(\n",
    "    engine=gpt_model,\n",
    "    messages=messages,\n",
    "    functions=sum,\n",
    "    function_call=\"auto\",\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8WveEzI0uoPJflKYPK11vctJ9gOlj at 0x1eecc281f10> JSON: {\n",
       "  \"id\": \"chatcmpl-8WveEzI0uoPJflKYPK11vctJ9gOlj\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1702858786,\n",
       "  \"model\": \"gpt-35-turbo-16k\",\n",
       "  \"prompt_filter_results\": [\n",
       "    {\n",
       "      \"prompt_index\": 0,\n",
       "      \"content_filter_results\": {\n",
       "        \"hate\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"self_harm\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"sexual\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"violence\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"function_call\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"function_call\": {\n",
       "          \"name\": \"sum\",\n",
       "          \"arguments\": \"{\\n  \\\"a\\\": 3,\\n  \\\"b\\\": 5\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 63,\n",
       "    \"completion_tokens\": 21,\n",
       "    \"total_tokens\": 84\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"sum\",\n",
      "    \"arguments\": \"{\\n  \\\"a\\\": 3,\\n  \\\"b\\\": 5\\n}\"\n",
      "  }\n",
      "} \n",
      "\n",
      "\n",
      "Function call:\n",
      "{\n",
      "  \"name\": \"sum\",\n",
      "  \"arguments\": \"{\\n  \\\"a\\\": 3,\\n  \\\"b\\\": 5\\n}\"\n",
      "} \n",
      "\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"a\": 3,\n",
      "  \"b\": 5\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response:\")\n",
    "print(response.choices[0].message, \"\\n\")\n",
    "print(\"\\nFunction call:\")\n",
    "print(response.choices[0].message.function_call, \"\\n\")\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=section_two></a>\n",
    "**Python to JSON Function Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farza\\AppData\\Local\\Temp\\ipykernel_14316\\3158027094.py:25: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'sum',\n",
       " 'description': 'Adds a + b',\n",
       " 'parameters': {'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
       "   'b': {'default': 1, 'title': 'B', 'type': 'integer'}},\n",
       "  'required': ['a'],\n",
       "  'title': 'Input for `sum`',\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum(a:int, b:int=1):\n",
    "    \"Adds a + b\"\n",
    "    return a + b\n",
    "\n",
    "def subtract(a:int, b:int=1):\n",
    "    \"Subtracts a - b\"\n",
    "    return a - b\n",
    "\n",
    "def multiply(a:int, b:int=1):\n",
    "    \"Multiplies a * b\"\n",
    "    return a * b\n",
    "\n",
    "def jsonschema(f):\n",
    "        \"\"\"\n",
    "        Generate a JSON schema for the input parameters of the given function.\n",
    "\n",
    "        Parameters:\n",
    "            f (FunctionType): The function for which to generate the JSON schema.\n",
    "\n",
    "        Returns:\n",
    "            Dict: A dictionary containing the function name, description, and parameters schema.\n",
    "        \"\"\"\n",
    "        kw = {n: (o.annotation, ... if o.default == Parameter.empty else o.default)\n",
    "              for n, o in inspect.signature(f).parameters.items()}\n",
    "        s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n",
    "        return dict(name=f.__name__, description=f.__doc__, parameters=s)\n",
    "\n",
    "def execute_json_function(response):\n",
    "        \"\"\"\n",
    "        Execute a function based on the response from an OpenAI ChatCompletion API call.\n",
    "\n",
    "        Parameters:\n",
    "            response: The response object from the OpenAI ChatCompletion API call.\n",
    "\n",
    "        Returns:\n",
    "            List: The result of the executed function.\n",
    "        \"\"\"\n",
    "        func_name = response.choices[0].message.function_call.name\n",
    "        func_args = json.loads(\n",
    "            response.choices[0].message.function_call.arguments)\n",
    "        # Call the function with the given arguments\n",
    "        if func_name == 'sum':\n",
    "            result = sum(**func_args)\n",
    "        elif func_name == 'subtract':\n",
    "            result = subtract(**func_args)\n",
    "        elif func_name == 'multiply':\n",
    "            result = multiply(**func_args)\n",
    "        else:\n",
    "            raise ValueError(f\"Function '{func_name}' not found.\")\n",
    "        return result\n",
    "\n",
    "print(type(jsonschema(sum)), \"\\n\")\n",
    "jsonschema(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When functions are provided, by default the function_call will be set to \"auto\" and the model will decide whether or not a function should be called. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farza\\AppData\\Local\\Temp\\ipykernel_14316\\3158027094.py:25: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  s = create_model(f'Input for `{f.__name__}`', **kw).schema()\n"
     ]
    }
   ],
   "source": [
    "llm_system_role = \"Answer the user's question.\"\n",
    "prompt = \"What is 6*3?\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "            engine=gpt_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": llm_system_role},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature,\n",
    "            functions=[jsonschema(sum), jsonschema(subtract), jsonschema(multiply)],\n",
    "            function_call=\"auto\",            \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-8Wvhlp9NyN8PRY04mhopzradCgW83 at 0x1eecc282c90> JSON: {\n",
       "  \"id\": \"chatcmpl-8Wvhlp9NyN8PRY04mhopzradCgW83\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"created\": 1702859005,\n",
       "  \"model\": \"gpt-35-turbo-16k\",\n",
       "  \"prompt_filter_results\": [\n",
       "    {\n",
       "      \"prompt_index\": 0,\n",
       "      \"content_filter_results\": {\n",
       "        \"hate\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"self_harm\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"sexual\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        },\n",
       "        \"violence\": {\n",
       "          \"filtered\": false,\n",
       "          \"severity\": \"safe\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"function_call\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"role\": \"assistant\",\n",
       "        \"function_call\": {\n",
       "          \"name\": \"multiply\",\n",
       "          \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
       "        }\n",
       "      },\n",
       "      \"content_filter_results\": {}\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 128,\n",
       "    \"completion_tokens\": 21,\n",
       "    \"total_tokens\": 149\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response:\n",
      "{\n",
      "  \"role\": \"assistant\",\n",
      "  \"function_call\": {\n",
      "    \"name\": \"multiply\",\n",
      "    \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
      "  }\n",
      "} \n",
      "\n",
      "\n",
      "Function call:\n",
      "{\n",
      "  \"name\": \"multiply\",\n",
      "  \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
      "} \n",
      "\n",
      "\n",
      "Function call arguments:\n",
      "{\n",
      "  \"a\": 6,\n",
      "  \"b\": 3\n",
      "} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Full response:\")\n",
    "print(response.choices[0].message, \"\\n\")\n",
    "print(\"\\nFunction call:\")\n",
    "print(response.choices[0].message.function_call, \"\\n\")\n",
    "print(\"\\nFunction call arguments:\")\n",
    "print(response.choices[0].message.function_call.arguments, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1eecc2830b0> JSON: {\n",
       "  \"name\": \"multiply\",\n",
       "  \"arguments\": \"{\\n  \\\"a\\\": 6,\\n  \\\"b\\\": 3\\n}\"\n",
       "}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execute_json_function(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-RT-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
